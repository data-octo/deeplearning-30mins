{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[](https://storage.googleapis.com/gweb-cloudblog-publish/original_images/neural-network-18m7jp.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TensorFlow Playground](https://playground.tensorflow.org), a web app written in JavaScript that lets you play with a real neural network running in your browser and click buttons and tweak parameters to see how it works. You can play with TensorFlow Playground so that you can understand the core ideas behind neural networks. Then you can understand why people have become so excited by the technology as of late.\n",
    "\n",
    "![title](images/tensorflow-playground.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem type : classification or regression\n",
    "\n",
    "In classification problem, the output is supposed to be discrete value like a label (0 or 1, red, green, or blue, cat or dog etc…). In regression problem, the output is supposed to be continuous value (-0.1234, -0.0012, 1.2345, 12345 etc…)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features : X1, X2, X1², X2², X1X2, sin(X1), sin(X2)…\n",
    "\n",
    "A feature is “an individual measurable property of a phenomenon being observed”. We can assume X1 as a collection of features that contains different n individual features than X2. We can apply mathematical operations before feeding it in like the list of features X1², X2², X1X2, sin(X1), and sin(X2) to make the model more powerful. Normalize entire features? Reduce the number of features? Generate new features from the currently available features? Degree of freedom on this hyperparams is large and its impact is so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of hidden layers : 0…\n",
    "\n",
    "A hidden layer is an intermediate set of neurons made by mapping weights and non-linear transformation (activation function) onto neurons in the previous layer. If your data is simple like you can separate them by drawing a straight line, you wont need any hidden layer. Beyond that, it is said one hidden layer is sufficient for the large majority of conventional problem. However, we need more of them when we apply it to more complex problems that we are going to explore.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of neurons in each hidden layer : 1…\n",
    "\n",
    "This means the number of neurons in a layer. In a layer, each neuron holds different intermediate computational values since each is mapped with different weights configuration. According to the author of “Introduction to Neural Networks for Java”, Jeff Heaton, ‘the optimal size of the hidden layer is usually between the size of the input and size of the output layers’. This only can be applied for simpler and conventional model of neural nets though, it is good guidance to consider often times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning Rate : 0.00001, 0.0001, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10…\n",
    "\n",
    "Step size of adjustments of weights/parameters in each iteration. Though how it impacts is different between machine learning architectures, it should be optimized to find its best value to achieve faster training process and more accurate result. Higher value give faster learning but it may cause the model to fail to converge. Lower value gives higher chance to converge but may cause the model to learn too slow and even stuck in a local minima/maxima. Step size should be fixed? No, over time we may vary it. It is one of techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation function : ReLU, Tanh, Sigmoid, Linear…\n",
    "\n",
    "Different activation function changes how we convert weighted values through synapses for a better training. Generally speaking, ReLU is a primary choice (avoid gradient vanishing, efficient without pre-training in deep neural nets). Leaky ReLU or Maxout comes second and Tanh third choice. Sigmoid and linear cannot be used if the output should be multi-class. Activation functions are really interesting topic when we microscope it actually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization : L1, L2…\n",
    "\n",
    "Regularization controls the capacity of neural nets to prevent overfitting, which a model gets too optimized for the training data set and loose generality to predict accurately for new data. Sampling data is constrained in real-world. There’s no perfect data set and often times it contains ill-posed variance. Regularization injects our preferences on weights into available data in order to mitigate the real-world sampling limits. Or just we suspect the model is too powerful, we can introduce regularization as a counterforce against it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters\n",
    "\n",
    "Each hyperparameters are there for making the machine learning, in particular the above set of them; the neural nets learning, better. In practice, these hyperparameters are mutual dependent and by combining them produces much different results. We would not know the best set of tuning values without understanding mutual impacts. However, for the sake of learning, starting learning one hyperparmeter at a step will help you to become a wizard of neural nets faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
