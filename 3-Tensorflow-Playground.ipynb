{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[](https://storage.googleapis.com/gweb-cloudblog-publish/original_images/neural-network-18m7jp.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "[TensorFlow Playground](https://playground.tensorflow.org), a web app written in JavaScript that lets you play with a real neural network running in your browser and click buttons and tweak parameters to see how it works. You can play with TensorFlow Playground so that you can understand the core ideas behind neural networks. Then you can understand why people have become so excited by the technology as of late.\n",
    "\n",
    "For now we just need to know what elements are inside TensorFlow and what they mean. In later tutorials we'll cover them in more details.\n",
    "\n",
    "![image](images/tensorflow-playground.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents\n",
    "\n",
    "#### 1 Run \n",
    "\n",
    "After clicking on the run button with default parameters, you will see the Neural Network classify the dots into blue and orange."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 Output\n",
    "\n",
    "This is the visual result after the Neural Network run with the defined parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 Iterations\n",
    "\n",
    "When we run the Neural Network, the computer is trying to find the best combination of weights and threshold. The more iterations it run, the better combination the comupter can find."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 Learning Rate\n",
    "\n",
    "Learning rate is step size of adjustments of weights/parameters in each iteration and it determines how quickly or how slowly you want to update your weight(parameter) values.  Though how it impacts is different between machine learning architectures, it should be optimized to find its best value to achieve faster training process and more accurate result. Higher value give faster learning but it may cause the model to fail to converge. Lower value gives higher chance to converge but may cause the model to learn too slow and even stuck in a local minima/maxima.\n",
    "\n",
    "![image](images/learning-rate.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 Activation\n",
    "\n",
    "Activation functions are used to introduce non-linearity to neural networks. There are many activation functions used in deep learning industry and ReLU, SeLU and TanH are preferred over Sigmoid activation function.\n",
    "\n",
    "![image](images/linear-nonlinear.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6 Regularization\n",
    "\n",
    "It is used to overcome the over-fitting problem. In regularization we penalise our loss term by adding a L1 (LASSO) or an L2(Ridge) norm on the weight vector w (it is the vector of the learned parameters in the given algorithm).\n",
    "\n",
    "L1 regularization, where the cost added is proportional to the absolute value of the weights coefficients (i.e. to what is called the \"L1 norm\" of the weights).\n",
    "\n",
    "L2 regularization, where the cost added is proportional to the square of the value of the weights coefficients (i.e. to what is called the \"L2 norm\" of the weights). L2 regularization is also called weight decay in the context of neural networks. Don't let the different name confuse you: weight decay is mathematically the exact same as L2 regularization.\n",
    "\n",
    "![image](images/overfitting.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7 Regularization Rate\n",
    "\n",
    "The number for Regularization functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8 Problem type \n",
    "\n",
    "There are two types: Classification or Regression. In classification problem, the output is supposed to be discrete value like a label (0 or 1, red, green, or blue, cat or dog etc…). In regression problem, the output is supposed to be continuous value (-0.1234, -0.0012, 1.2345, 12345 etc…)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9 Features\n",
    "\n",
    "A feature is “an individual measurable property of a phenomenon being observed”. We can assume X1 as a collection of features that contains different n individual features than X2. We can apply mathematical operations before feeding it in like the list of features X1², X2², X1X2, sin(X1), and sin(X2) to make the model more powerful. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10 Hidden Layers\n",
    "\n",
    "Hidden layers have neurons(nodes) which apply different transformations to the input data. The number of the Hidden Layers is also called the depth of the network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11 Data\n",
    "\n",
    "We have six different data sets Circle, Exclusive OR (XOR), Gaussian, Spiral, plane and multi Gaussian. The first four are for classification problems and last two are for regression problems. Small circles are the data points which correspond to positive one and negative one. In general, positive values are shown in blue and negative in orange.\n",
    "\n",
    "In the hidden layers, the lines are colored by the weights of the connections between neurons. Blue shows a positive weight, which means the network is using that output of the neuron as given. An orange line shows that the network is assigning a negative weight.\n",
    "\n",
    "In the output layer, the dots are colored orange or blue depending on their original values. The background color shows what the network is predicting for a particular area. The intensity of the color shows how confident that prediction is.\n",
    "\n",
    "The data can have noise. Noisy data can be caused by hardware failures, programming errors and gibberish input from speech or optical character recognition (OCR) programs. Batch Size is the number of training examples in one forward/backward pass. The higher the batch size, the more memory space you’ll need. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12 Neurons\n",
    "\n",
    "This means the number of neurons in a layer. In a layer, each neuron holds different intermediate computational values since each is mapped with different weights configuration. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "Congrats! If you finish reading this tutorial and manage to remember all these terminologies, you'll be able to discuss with Neural Network professionals with confidence. \n",
    "\n",
    "All settings above are Hyperparameters, which means they are pre-defined but not trained.In practice, these hyperparameters are mutual dependent and by combining them produces much different results. We would not know the best set of tuning values without understanding mutual impacts. We'll cover them in details in later tutorials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Reading\n",
    "\n",
    "* [Web: Understanding neural networks with TensorFlow Playground](https://cloud.google.com/blog/products/gcp/understanding-neural-networks-with-tensorflow-playground)\n",
    "* [Video: Deep Learning Lecture 3: Hands-On in the Playground](https://www.youtube.com/watch?v=ru9dXF04iSE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
